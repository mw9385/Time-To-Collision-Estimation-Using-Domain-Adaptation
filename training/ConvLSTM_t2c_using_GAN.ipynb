{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2\n",
    "sys.path.append('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import keras.backend as K\n",
    "\n",
    "from PIL import Image                              \n",
    "from keras import optimizers, losses, metrics, models\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from models.cnn_model_LSTM_many_to_one import cnn_lstm  \n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "root_path = '/data/data_2021_08_18/preprocessed/CYCLEGAN_CONVLSTM_DATA/model_V2/'\n",
    "img_dir = root_path + '/train_image_ConvLSTM_2021_08_18_combined.npy'\n",
    "label_dir = root_path + '/train_label_ConvLSTM_2021_08_18_combined.npy'\n",
    "# Allocate variable for train_x and train_y\n",
    "img_x = np.load(img_dir)\n",
    "label = np.load(label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the label\n",
    "n = np.arange(len(label))\n",
    "plt.figure(figsize = (16,9))\n",
    "plt.scatter(n, label[n])\n",
    "print(np.sum(label<6))\n",
    "plt.ylim([0,1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualization of training data\n",
    "n = np.random.randint(len(img_x))\n",
    "print(\"True time to collision: {}\".format(label[n]))\n",
    "\n",
    "sample = img_x[n]\n",
    "print(\"Shape of the data:{}\".format(np.shape(sample)))\n",
    "sample_y = label[n]\n",
    "fig = plt.figure(figsize = (10,8))\n",
    "for i in range(6):\n",
    "    _sample = np.reshape(sample[i,:],[128, 128,3])\n",
    "    ax = fig.add_subplot(2,3,i+1)\n",
    "    ax.imshow(_sample)\n",
    "    ax.set_title('time step:' + str(i))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.02, hspace=0)\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig('sample_input.png', dpi = 600, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split for train, validation, and test\n",
    "\n",
    "n = np.int(len(img_x)* 0.80)\n",
    "m = np.int(len(label)*0.97)\n",
    "\n",
    "# train \n",
    "train_img = img_x[:n] \n",
    "train_y = label[:n]\n",
    "\n",
    "# validation\n",
    "valid_img = img_x[n:m]\n",
    "valid_y = label[n:m]\n",
    "\n",
    "# test\n",
    "test_img = img_x[m:]\n",
    "test_y = label[m:]\n",
    "\n",
    "#check size\n",
    "print(\"Training data size: {}\".format(np.shape(train_img)))\n",
    "print(\"Training y size: {}\".format(np.shape(train_y)))\n",
    "print(\"Validation data size: {}\".format(np.shape(valid_img)))\n",
    "print(\"Test data size: {}\".format(np.shape(test_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del img_x\n",
    "del label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of training data\n",
    "n = np.random.randint(len(train_img))\n",
    "print(\"True time to collision: {}\".format(train_y[n]))\n",
    "\n",
    "sample = train_img[n]\n",
    "print(\"Shape of the data:{}\".format(np.shape(sample)))\n",
    "sample_y = train_y[n]\n",
    "fig = plt.figure(figsize = (10,8))\n",
    "for i in range(6):\n",
    "    _sample = np.reshape(sample[i,:],[128, 128,3])\n",
    "    ax = fig.add_subplot(2,3,i+1)\n",
    "    ax.imshow(_sample)\n",
    "    ax.set_title('time step:' + str(i))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.02, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(img_width, img_height, img_channel, num_frames, output_dim,num_actions, model_path):\n",
    "    model = cnn_lstm(img_width, img_height, img_channel, num_frames, output_dim, num_actions)\n",
    "    print(model_path)\n",
    "    if model_path is not None:\n",
    "        model.load_weights(model_path)\n",
    "        print(\"Loaded model from {}\".format(model_path))\n",
    "    else:\n",
    "        print(\"Impossible to find weight path. Train new model.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_loss(log_var, mean):\n",
    "    def customLoss(yTrue, yPred):\n",
    "        loss1 = K.exp(-log_var) * K.square(K.abs(yTrue - mean))\n",
    "        return loss1\n",
    "    return customLoss\n",
    "    \n",
    "def var_loss(log_var):\n",
    "    def customLoss(yTrue, yPred):\n",
    "        loss2 = log_var\n",
    "        return loss2\n",
    "    return customLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and optimizer\n",
    "# single GPU\n",
    "# est_coll_model = getModel(128, 128, 3, 6, 1,8,  'None')\n",
    "# multiple GPU\n",
    "# model_path = './cnn_saved_models/2021_08_18_GAN_model/ConvLSTM_gan_model_V10.h5'\n",
    "est_coll_model = multi_gpu_model(getModel(128, 128, 3, 6, 1,8,  None), gpus=2)\n",
    "# load weight\n",
    "# est_coll_model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_layer = est_coll_model.get_layer('variance').output\n",
    "mean_layer = est_coll_model.get_layer('mean').output\n",
    "opti = optimizers.Adam(lr= 1e-6, decay = 1e-5)\n",
    "est_coll_model.compile(optimizer= opti, loss = [mean_loss(var_layer, mean_layer), var_loss(var_layer)], loss_weights = [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [    \n",
    "    keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience= 50,\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(\n",
    "    log_dir = 'log_dir/2021_08_18_GAN_model/ConvLSTM_gan_model_V10_pretrained_V2_further_tranining',\n",
    "    histogram_freq=1,),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./cnn_saved_models/2021_08_18_GAN_model/ConvLSTM_gan_model_V10_pretrained_V2_further_tranining.h5',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1)    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = est_coll_model.fit(train_img, [train_y, train_y],\n",
    "                             validation_data=[valid_img ,[valid_y, valid_y]],\n",
    "                             epochs=1500,\n",
    "                             batch_size=32,\n",
    "                             callbacks = callbacks_list,\n",
    "                            initial_epoch= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the performance of the proposed model\n",
    "import time\n",
    "error_matrix = []\n",
    "for j in range(1000):\n",
    "    print(j)\n",
    "    st = time.time()\n",
    "    sample = test_img[j]\n",
    "    _sample_img = np.reshape(sample, [-1, 6, 128, 128, 3])\n",
    "    \n",
    "    y_eval = []\n",
    "    aleatoric = []\n",
    "    epistemic = [] \n",
    "    \n",
    "    for i in range(5):   \n",
    "        temp_input = np.reshape(test_img[j], [-1, 6, 128, 128, 3])    \n",
    "        _mean, _var = est_coll_model.predict(temp_input)    \n",
    "        y_eval.append(_mean)    \n",
    "        epistemic.append(_mean)\n",
    "        aleatoric.append(_var)   \n",
    "\n",
    "    epistemic_uncertainty = np.sqrt(np.var(epistemic))\n",
    "    aleatoric_uncertainty = np.sqrt(np.mean((np.exp(aleatoric))))\n",
    "    total_uncertainty = epistemic_uncertainty + aleatoric_uncertainty\n",
    "    y_eval = np.mean(y_eval)\n",
    "            \n",
    "    sample = test_img[j]\n",
    "    sample_y = test_y[j]\n",
    "    error_matrix.append(np.abs(sample_y - y_eval))    \n",
    "\n",
    "#     fig = plt.figure(figsize = (10,8))\n",
    "#     for i in range(6):\n",
    "#         _sample = np.reshape(sample[0,:],[128, 128,3])\n",
    "#         ax = fig.add_subplot(2,3,i+1)\n",
    "#         ax.imshow(_sample)\n",
    "#         ax.set_title('time step:' + str(i))\n",
    "#         ax.axis(\"off\")\n",
    "\n",
    "#     plt.subplots_adjust(wspace=0.02, hspace=0)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.vlines(sample_y,ymin=0, ymax=6, color = 'r', label='true')\n",
    "#     plt.vlines(y_eval, ymin=0,ymax=6, color='b', label='estimate')\n",
    "#     plt.axvspan(y_eval - aleatoric_uncertainty, y_eval + aleatoric_uncertainty, ymin=0, ymax=6, color='green', alpha=0.3, label = 'aleatoric')\n",
    "#     plt.axvspan(y_eval - epistemic_uncertainty, y_eval + epistemic_uncertainty, ymin=0, ymax=6, color='blue', alpha=0.3, label = 'epistemic')\n",
    "#     plt.axvspan(y_eval - total_uncertainty, y_eval + total_uncertainty, ymin=0, ymax=6, color='grey', alpha=0.6, label = 'total uncertainty')\n",
    "#     plt.xlim([-1,6])\n",
    "#     plt.legend()\n",
    "#     plt.grid()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Total error:{}'.format(np.mean(error_matrix)))\n",
    "print('Variance:{}'.format(np.std(error_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error on test dataset = 0.15 sec (389 samples) / computation time = 0.185sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021_08_18: ConvLSTM_V3.h5 is the best (it is used as testing model for the paper)\n",
    "# load_model\n",
    "# est_coll_model.load_weights('./cnn_saved_models/GAN_model/ConvLSTM_V3.h5')\n",
    "est_coll_model.load_weights('./cnn_saved_models/2021_08_18_GAN_model/ConvLSTM_gan_model_V10_pretrained_V2_further_tranining.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 20 episode of parrot experiments\n",
    "- Approximate velocity of the parrot: 0.2m/s (Gazebo model velocity: 0.5m/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for episode in range(20):\n",
    "    print(episode)\n",
    "    \n",
    "    # Check the model performance using totally unseen data\n",
    "    root_path = '/home/asl/machine_learning/time_to_collision/Preprocessed_data/Parrot_data/image_new'\n",
    "#     new_img_dir = root_path + '/img_' + str(17) + '.npy'\n",
    "#     new_label_dir = root_path + '/label_' + str(17) + '.npy'\n",
    "    new_img_dir = root_path + '/img_' + str(episode) + '.npy'\n",
    "    new_label_dir = root_path + '/label_' + str(episode) + '.npy'\n",
    "\n",
    "    # Allocate variable for train_x and train_y\n",
    "    new_test_img = np.load(new_img_dir) / 255.\n",
    "    new_test_y = np.load(new_label_dir)\n",
    "    new_test_y = new_test_y # give offset for wrong crash distance\n",
    "\n",
    "    label_index = new_test_y > 6\n",
    "    new_test_y[label_index] = 6\n",
    "\n",
    "    # For visualizing t2c values    \n",
    "    true_label = []\n",
    "    estimated_label = []  \n",
    "    \n",
    "    # visualization of training data\n",
    "    error_matrix = []\n",
    "    uncertainty_matrix = []\n",
    "    print(len(new_test_img))\n",
    "    \n",
    "    for j in range(len(new_test_img)):\n",
    "        sample = new_test_img[j]    \n",
    "        temp_input = np.reshape(sample, [-1, 6, 128, 128, 3])\n",
    "\n",
    "        y_eval = []\n",
    "        aleatoric = []\n",
    "        epistemic = []\n",
    "\n",
    "        for i in range(5):\n",
    "            _mean, _var = est_coll_model.predict(temp_input)    \n",
    "            y_eval.append(_mean)    \n",
    "            epistemic.append(_mean)\n",
    "            aleatoric.append(_var)   \n",
    "\n",
    "        epistemic_uncertainty = np.sqrt(np.var(epistemic))\n",
    "        aleatoric_uncertainty = np.sqrt(np.mean((np.exp(aleatoric))))\n",
    "        total_uncertainty = epistemic_uncertainty + aleatoric_uncertainty\n",
    "        y_eval = np.mean(y_eval)\n",
    "        uncertainty_matrix.append(total_uncertainty - 0.2)    \n",
    "\n",
    "        sample = new_test_img[j]\n",
    "        sample = np.reshape(sample, [6,128, 128,3])\n",
    "        sample_y = new_test_y[j]\n",
    "        error_matrix.append(np.abs(sample_y - y_eval))\n",
    "        \n",
    "        # store true and estimated values \n",
    "        true_label.append(sample_y)\n",
    "        estimated_label.append(y_eval)\n",
    "              \n",
    "        fig = plt.figure(figsize = (10,8))\n",
    "        for i in range(6):\n",
    "            _sample = np.reshape(sample[i], [128, 128,3])\n",
    "            ax = fig.add_subplot(2,3,i+1)\n",
    "            ax.imshow(_sample)\n",
    "            ax.set_title('Time step:' + str(i+j), fontsize = 25)\n",
    "            ax.axis('off')\n",
    "        plt.subplots_adjust(wspace=0.02, hspace=0)\n",
    "        plt.show()        \n",
    "#         fig.savefig('./Results/Figures_for_paper/image_' + str(j) + '.png', dpi = 600, bbox_inches = 'tight')\n",
    "\n",
    "        fig3 = plt.figure()\n",
    "#         plt.vlines(sample_y,ymin=0, ymax=6, color = 'r', marker = '--', label='true')\n",
    "        plt.vlines(sample_y,ymin=0, ymax=6, color= 'r', linestyle='dashed', linewidth=2,label='true')\n",
    "        plt.vlines(y_eval, ymin=0,ymax=6, color='b', linewidth=2, label='estimate')\n",
    "        plt.axvspan(y_eval - aleatoric_uncertainty, y_eval + aleatoric_uncertainty, ymin=0, ymax=6, color='green', alpha=0.6, label = 'aleatoric')\n",
    "        plt.axvspan(y_eval - epistemic_uncertainty, y_eval + epistemic_uncertainty, ymin=0, ymax=6, color='blue', alpha=0.6, label = 'epistemic')\n",
    "        plt.axvspan(y_eval - total_uncertainty, y_eval + total_uncertainty, ymin=0, ymax=6, color='grey', alpha=0.6, label = 'total uncertainty')\n",
    "        plt.xlim([0,6])\n",
    "        plt.ylim([0,1])\n",
    "        plt.xlabel('Time-To-Collision [sec.]', fontsize = 12, fontweight='bold')\n",
    "        plt.legend(fontsize = 12)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "#         fig3.savefig('./Results/Figures_for_paper/TTC_' + str(j) + '.png', dpi = 600, bbox_inches = 'tight')\n",
    "        \n",
    "    \n",
    "    total_error.append(np.mean(error_matrix))\n",
    "    print(\"Total_error:{}\".format(np.mean(error_matrix)))\n",
    "    print(\"Error variance:{}\".format(np.var(error_matrix)))\n",
    "    print(\"Uncertainty:{}\".format(np.mean(uncertainty_matrix)))\n",
    "    \n",
    "    fig2 = plt.figure(figsize=(10,8))\n",
    "    plt.errorbar(np.arange(len(estimated_label)), estimated_label, yerr = uncertainty_matrix, marker = 'o', label = 'estimated')\n",
    "    plt.errorbar(np.arange(len(true_label)), true_label, label = 'true')\n",
    "    plt.legend(fontsize = '20')\n",
    "    plt.grid()\n",
    "    plt.ylabel('Time to collision', fontsize = '40')\n",
    "    plt.xlabel('Number of frames', fontsize = '40')\n",
    "    plt.xticks(fontsize=25)    \n",
    "    plt.yticks(fontsize=25)\n",
    "    plt.show()\n",
    "#     fig2.savefig('./experiment_results' + str(episode) + '.png', dpi = 600, bbox_inches = 'tight')\n",
    "    \n",
    "    \n",
    "print(np.mean(total_error))\n",
    "print(np.std(total_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model ConvLSTM_gan_model_V10.h5 accuracy:\n",
    "# mean:1.85\n",
    "# std:0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for episode in range(1):\n",
    "    print(episode)\n",
    "    \n",
    "    # Check the model performance using totally unseen data\n",
    "    root_path = './data_collection/real_test_images/preprocessed'\n",
    "    new_img_dir = root_path + '/episode_0/image.npy'    \n",
    "    new_label_dir = root_path + '/ttc/distance_0.npy'\n",
    "    new_velocity_dir = root_path + '/velocity/velocity_0.npy'\n",
    "\n",
    "    # Allocate variable for train_x and train_y\n",
    "    new_test_img = np.load(new_img_dir)\n",
    "    new_test_y = np.load(new_label_dir)\n",
    "    new_velocity_y = np.load(new_velocity_dir)        \n",
    "\n",
    "    # For visualizing t2c values    \n",
    "    true_label = []\n",
    "    estimated_label = []  \n",
    "    \n",
    "    # visualization of training data\n",
    "    error_matrix = []\n",
    "    uncertainty_matrix = []\n",
    "\n",
    "    for j in range(len(new_test_img)):\n",
    "        sample = new_test_img[j]    \n",
    "        temp_input = np.reshape(sample, [-1, 6, 128, 128, 3])\n",
    "\n",
    "        y_eval = []\n",
    "        aleatoric = []\n",
    "        epistemic = []\n",
    "\n",
    "        for i in range(5):\n",
    "            _mean, _var = est_coll_model.predict(temp_input)    \n",
    "            y_eval.append(_mean)    \n",
    "            epistemic.append(_mean)\n",
    "            aleatoric.append(_var)   \n",
    "\n",
    "        epistemic_uncertainty = np.sqrt(np.var(epistemic))\n",
    "        aleatoric_uncertainty = np.sqrt(np.mean((np.exp(aleatoric))))\n",
    "        total_uncertainty = epistemic_uncertainty + aleatoric_uncertainty\n",
    "        y_eval = np.mean(y_eval)\n",
    "        uncertainty_matrix.append(total_uncertainty)    \n",
    "\n",
    "        sample = new_test_img[j]\n",
    "        sample = np.reshape(sample, [6,128, 128,3])\n",
    "        sample_y = new_test_y[j]\n",
    "        error_matrix.append(np.abs(sample_y - y_eval))\n",
    "        \n",
    "        # store true and estimated values \n",
    "        true_label.append(sample_y)\n",
    "        estimated_label.append(y_eval)\n",
    "              \n",
    "        fig = plt.figure(figsize = (10,8))\n",
    "        for i in range(6):\n",
    "            _sample = np.reshape(sample[i], [128, 128,3])\n",
    "            ax = fig.add_subplot(2,3,i+1)\n",
    "            ax.imshow(_sample)\n",
    "            ax.set_title('Time step:t_' + str(i))\n",
    "            ax.axis('off')\n",
    "        plt.subplots_adjust(wspace=0.02, hspace=0)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.vlines(sample_y,ymin=0, ymax=6, color = 'r', label='true')\n",
    "        plt.vlines(y_eval, ymin=0,ymax=6, color='b', label='estimate')\n",
    "        plt.axvspan(y_eval - aleatoric_uncertainty, y_eval + aleatoric_uncertainty, ymin=0, ymax=6, color='green', alpha=0.3, label = 'aleatoric')\n",
    "        plt.axvspan(y_eval - epistemic_uncertainty, y_eval + epistemic_uncertainty, ymin=0, ymax=6, color='blue', alpha=0.3, label = 'epistemic')\n",
    "        plt.axvspan(y_eval - total_uncertainty, y_eval + total_uncertainty, ymin=0, ymax=6, color='grey', alpha=0.6, label = 'total uncertainty')\n",
    "        plt.xlim([0,6])\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    total_error.append(np.mean(error_matrix))\n",
    "    print(\"Total_error:{}\".format(np.mean(error_matrix)))\n",
    "    print(\"Error variance:{}\".format(np.var(error_matrix)))\n",
    "    print(\"Uncertainty:{}\".format(np.mean(uncertainty_matrix)))\n",
    "    \n",
    "    fig2 = plt.figure(figsize=(10,8))\n",
    "    plt.errorbar(np.arange(len(estimated_label)), estimated_label, yerr = uncertainty_matrix, marker = 'o', label = 'estimated')\n",
    "    plt.errorbar(np.arange(len(true_label)), true_label, label = 'true')\n",
    "    plt.legend(fontsize = '20')\n",
    "    plt.grid()\n",
    "    plt.ylabel('time to collision', fontsize = '40')\n",
    "    plt.xlabel('number of frames', fontsize = '40')\n",
    "    plt.show()\n",
    "#     fig2.savefig('./experiment_results' + str(episode) + '.png', dpi = 600, bbox_inches = 'tight')\n",
    "    \n",
    "print(np.mean(total_error))\n",
    "print(np.std(total_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error: 1.60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
